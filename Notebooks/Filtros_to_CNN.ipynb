{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Image Filtering'></a>\n",
    "# Image Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(directory):\n",
    "    # Verificar si el directorio existe\n",
    "    if not os.path.exists(directory):\n",
    "        print(\"El directorio no existe.\")\n",
    "        return\n",
    "\n",
    "    images = []\n",
    "    # Leer todas las im√°genes en el directorio\n",
    "    for file_name in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "            image = image.astype(np.uint8)\n",
    "            if image is not None:\n",
    "                images.append(image)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = read_images(\"Imagenes_1_2_sust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_path, target_size=None):\n",
    "    image = tf.keras.preprocessing.image.load_img(image_path,\n",
    "                target_size=target_size)\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image = image.astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "image = read_image(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dimensions of the image are (4, 6, 1). \n",
    "\n",
    "That is, the image has a resolution of **4x6 pixels**, with **1 color channels (Red, Green and Blue)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following kernel performs the identity operation. The result of the convolution returns the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEbCAYAAADu9DJZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFLUlEQVR4nO3cMYobZwCG4X+E3Hn2AEKCVO4MuUVqX9aQPnXIFYIWHSAqd9GkSOwqCGeyeYf1Pk89v/gqvcyM0LQsyzIA4H+223oAAG+D4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJPZrD95ut3G5XMY8z2OappfcBMArsSzLuF6v43A4jN3u/j3M6uBcLpdxOp3WHgfgO3I+n8fxeLx7zergzPM8xhjj919/GA/v3+aTuU8fPm49AWBTz+Np/DI+f23CPauD8+Ux2sP73XiY32Zw9tO7rScAbOvvf+P8llcrb7MUAOQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABI7P/rB3z68HHsp3cvseXV+fny29YTNvfT4cetJwCvhDscABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgMR+7cFlWcYYYzyPpzGWF9vzqvxxvW09YXPPy9PWE4ANPY+/vgO+NOGeafmWq/7B4+PjOJ1Oa44C8J05n8/jeDzevWZ1cG6327hcLmOe5zFN06qBALxuy7KM6/U6DofD2O3uv6VZHRwA+Df8aACAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQOJPh0RSOf12t1wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "identity_kernel = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 0]\n",
    "])\n",
    "\n",
    "img = cv2.filter2D(images[0], -1, identity_kernel)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(5, 5))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following filter applies the [Box Blur] (https://en.wikipedia.org/wiki/Box_blur), where each pixel of the resulting image has a value equal to the mean of its pixel values neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEbCAYAAADu9DJZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAE60lEQVR4nO3cMW7cVhRA0cfBuCTUD6SdeAVZsFfgnQjgBlhKGKZxkiYYOIR9A9nn1Pwfr7sg/weX4ziOAYCf7PJ/DwDA70FwAEgIDgAJwQEgITgAJAQHgITgAJAQHAAS17ML7/f7bNs267rOsiw/ciYAPojjOGbf97ndbnO5PH6HOR2cbdvm5eXl7HIAfiGvr6/z/Pz88JnTwVnXdWZmPs8fc51PZ7cB4AN7n7f5Ol/+bsIjp4Pz12e063ya6yI4AL+lb3/j/J6jFZcGAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkLieXXgcx8zMvM/bzPHD5gHgA3mft5n5pwmPnA7Ovu8zM/N1vpzdAoBfxL7v8/T09PCZ5fieLP2L+/0+27bNuq6zLMupAQH42I7jmH3f53a7zeXy+JTmdHAA4L9waQCAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQOJPiV1D4JUen4wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "box_blur_filter = (1/9) * np.array([\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 1]\n",
    "])\n",
    "\n",
    "img = cv2.filter2D(images[0], -1, box_blur_filter)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(5, 5))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example shows a filter widely used in digital image processing: the [Gaussian filtering] (https://en.wikipedia.org/wiki/Gaussian_blur)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEbCAYAAADu9DJZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAE60lEQVR4nO3cMW7cVhRA0cfBuCTUD6SdeAVZsFfgnQjgBlhKGKZxkiYYOIR9A9nn1Pwfr7sg/weX4ziOAYCf7PJ/DwDA70FwAEgIDgAJwQEgITgAJAQHgITgAJAQHAAS17ML7/f7bNs267rOsiw/ciYAPojjOGbf97ndbnO5PH6HOR2cbdvm5eXl7HIAfiGvr6/z/Pz88JnTwVnXdWZmPs8fc51PZ7cB4AN7n7f5Ol/+bsIjp4Pz12e063ya6yI4AL+lb3/j/J6jFZcGAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkLieXXgcx8zMvM/bzPHD5gHgA3mft5n5pwmPnA7Ovu8zM/N1vpzdAoBfxL7v8/T09PCZ5fieLP2L+/0+27bNuq6zLMupAQH42I7jmH3f53a7zeXy+JTmdHAA4L9waQCAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQOJPiV1D4JUen4wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gaussian_blur_filter = (1/16) * np.array([\n",
    "    [1, 2, 1],\n",
    "    [2, 4, 2],\n",
    "    [1, 2, 1]\n",
    "])\n",
    "\n",
    "img = cv2.filter2D(images[0], -1, gaussian_blur_filter)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(5, 5))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can gaussian noise to the image and see the effects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sigma = 20\n",
    "n, m = images[0].shape\n",
    "img_noise = img + sigma*np.random.randn(n, m)\n",
    "fig, ax = plt.subplots(1, figsize=(5, 5))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.imshow(img_noise.astype(np.uint8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img1 = cv2.filter2D(img_noise, -1, gaussian_blur_filter)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "for i in range(ax.shape[0]):\n",
    "    for j in range(ax.shape[1]):\n",
    "        ax[i,j].set_xticks([])\n",
    "        ax[i,j].set_yticks([])\n",
    "        \n",
    "ax[0, 0].set_title('Original Image')\n",
    "ax[0, 0].imshow(images[0].astype(np.uint8));\n",
    "\n",
    "ax[0, 1].set_title('Noisy Image')\n",
    "ax[0, 1].imshow(img_noise.astype(np.uint8));\n",
    "\n",
    "ax[1, 0].set_title('Box Blur')\n",
    "ax[1, 0].imshow(cv2.filter2D(img_noise, -1, box_blur_filter).astype(np.uint8));\n",
    "\n",
    "ax[1, 1].set_title('Gaussian Blur')\n",
    "ax[1, 1].imshow(cv2.filter2D(img_noise, -1, gaussian_blur_filter).astype(np.uint8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following filter is used for edge detection in images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "edge_detection_filter = np.array([\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1],\n",
    "    [1, 0, -1]\n",
    "])\n",
    "\n",
    "img = cv2.filter2D(images[0], -1, edge_detection_filter)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(5, 5))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "edge_detection_filter = np.array([\n",
    "    [0, 1, 0],\n",
    "    [1, -4, 1],\n",
    "    [0, 1, 0]\n",
    "])\n",
    "\n",
    "img = cv2.filter2D(images[0], -1, edge_detection_filter)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(5, 5))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "edge_detection_filter = np.array([\n",
    "    [1, 1, 1],\n",
    "    [1, -8, 1],\n",
    "    [1, 1, 1]\n",
    "])\n",
    "\n",
    "img = cv2.filter2D(images[0], -1, edge_detection_filter)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(5, 5))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filter(image, filter, padding=0, strides=1):\n",
    "    import cv2 as cv\n",
    "    import scipy.signal as sig\n",
    "    import numpy as np\n",
    "\n",
    "    img = sig.convolve2d(image, filter, mode=\"valid\")\n",
    "    fig, ax = plt.subplots(1, figsize=(5, 5))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(img);\n",
    "    print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filter_tf(image, filter, padding='VALID', strides=1):\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    image_float32 = np.array(image).astype(np.float32)\n",
    "    filter_float32 = np.array(filter).astype(np.float32)\n",
    "    imag = tf.constant(tf.reshape(image_float32, (1, image.shape[0], image.shape[1], 1)), dtype=tf.float32)\n",
    "    filt = tf.constant(tf.reshape(filter_float32, (filter.shape[0], filter.shape[1], 1, 1)), dtype=tf.float32)\n",
    "    img = tf.nn.conv2d(imag, filt, strides=strides, padding=padding)[0, ..., 0]\n",
    "    fig, ax = plt.subplots(1, figsize=(5, 5))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(img);\n",
    "    print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filter_CNN(image, filters, strides=(1, 1), padding='valid'):\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # Obtener dimensiones de los datos de entrada y los filtros\n",
    "#    input_shape = image.get_shape().as_list()\n",
    "#    filter_shape = filters.get_shape().as_list()    \n",
    "    filter_shape = len(filters)\n",
    "    \n",
    "    image_float32 = np.array(image).astype(np.float32)\n",
    "    for i,filter in enumerate(filters): filters[i] = np.array(filter).astype(np.float32)\n",
    "    # en el input las dimensiones tienen que ser [None,pixels_x,pixels_y,RGB(3) o GrayScale(1)]\n",
    "    # en el filtro tiene que ser [kernel_x,kernel_y,RGB(3) o GrayScale(1),N¬∫ filtros]\n",
    "    inputs = tf.constant(tf.reshape(image_float32, (1, image.shape[0], image.shape[1], 1)), dtype=tf.float32)\n",
    "    filters = [tf.constant(tf.reshape(filter, (filter.shape[0], filter.shape[1], 1, 1)), dtype=tf.float32) for filter in filters]\n",
    "#    filters = tf.constant(tf.reshape(filter_float32, (filter.shape[0], filter.shape[1], 1, 1)), dtype=tf.float32)\n",
    "    \n",
    "    # Definir los strides y el padding\n",
    "    try:\n",
    "        strides = [1, strides[0], strides[1], 1]\n",
    "    except TypeError:\n",
    "        strides = [1, strides, strides, 1]\n",
    "    if padding == 'valid':\n",
    "        padding = 'VALID'\n",
    "    elif padding == 'same':\n",
    "        padding = 'SAME'\n",
    "\n",
    "    # Realizar la convoluci√≥n\n",
    "    for filter in filters:\n",
    "        img = tf.nn.conv2d(inputs, filter, strides=strides, padding=padding)[0,:,:,0] #el [0,:,:,0] sirve para eliminar las dos dimendiones marcadas tambien se puede poner [0,...,0]\n",
    "        fig, ax = plt.subplots(1, figsize=(5, 5))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(img);\n",
    "        print(img)\n",
    "\n",
    "# Ejemplo de uso\n",
    "#inputs = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])  # Datos de entrada\n",
    "#filters = tf.Variable(tf.random_normal([3, 3, 3, 64]))  # Filtros\n",
    "#conv = custom_conv2d(inputs, filters, strides=(1, 1), padding='same')\n",
    "\n",
    "# Verificar la forma del resultado\n",
    "#print(conv.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filter_CNN2(image, filters, strides=(1, 1), padding='valid'):\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # Obtener dimensiones de los datos de entrada y los filtros\n",
    "#    input_shape = image.get_shape().as_list()\n",
    "#    filter_shape = filters.get_shape().as_list()    \n",
    "    filter_shape = len(filters)\n",
    "    filter_height = filters[0].shape[0]\n",
    "    filter_width = filters[0].shape[1]\n",
    "    \n",
    "    image_float32 = np.array(image).astype(np.float32)\n",
    "    for i,filter in enumerate(filters): filters[i] = np.array(filter).astype(np.float32)\n",
    "    # en el input las dimensiones tienen que ser [None,pixels_x,pixels_y,RGB(3) o GrayScale(1)]\n",
    "    # en el filtro tiene que ser [kernel_x,kernel_y,RGB(3) o GrayScale(1),N¬∫ filtros]\n",
    "    inputs = tf.constant(tf.reshape(image_float32, (1, image.shape[0], image.shape[1], 1)), dtype=tf.float32)\n",
    "    filters = np.swapaxes(filters,0,1)\n",
    "    filters = np.swapaxes(filters,1,2)\n",
    "    filters_tf = tf.constant(tf.reshape(filters, (filter_height, filter_width, 1, filter_shape)), dtype=tf.float32)\n",
    "\n",
    "    # Definir los strides y el padding\n",
    "    try:\n",
    "        strides = [1, strides[0], strides[1], 1]\n",
    "    except TypeError:\n",
    "        strides = [1, strides, strides, 1]\n",
    "    if padding == 'valid':\n",
    "        padding = 'VALID'\n",
    "    elif padding == 'same':\n",
    "        padding = 'SAME'\n",
    "\n",
    "    # Realizar la convoluci√≥n\n",
    "    images_conv = tf.nn.conv2d(inputs, filters_tf, strides=strides, padding=padding) #el [0,:,:,0] sirve para eliminar las dos dimendiones marcadas tambien se puede poner [0,...,0]\n",
    "    print(images_conv.shape)\n",
    "    for i in range(images_conv.shape[3]):\n",
    "        print(filters_tf[:,:,0,i])\n",
    "        img = images_conv[0,:,:,i]\n",
    "        fig, ax = plt.subplots(1, figsize=(5, 5))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        print(img)\n",
    "\n",
    "# Ejemplo de uso\n",
    "#inputs = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])  # Datos de entrada\n",
    "#filters = tf.Variable(tf.random_normal([3, 3, 3, 64]))  # Filtros\n",
    "#conv = custom_conv2d(inputs, filters, strides=(1, 1), padding='same')\n",
    "\n",
    "# Verificar la forma del resultado\n",
    "#print(conv.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 4, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAADaCAYAAACSN/T8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEKklEQVR4nO3aMW7UWgCG0TujlDgR7SgRVK9GbII1swnEChAokhcQ0/E0pgoVGoEJ3yjMObWv9TejT7Znt67rOgDgL9ufewAAl0FwAEgIDgAJwQEgITgAJAQHgITgAJC42nrweDyOeZ7HNE1jt9s95SYAnol1XceyLONwOIz9/vQzzObgzPM87u7uth4H4B9yf38/bm9vT16zOTjTNI0xxvjy4fW4fuHNHMAlevh6HK/efv7RhFM2B+fxNdr1i/24ngQH4JL9yqcVpQAgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQOLq3AP4M+8Ob8494eK9nz+eewI8C55wAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEldbD67rOsYY4+Hr8cnG8Pv+X7+de8LFe1j8Brhcjw14bMIpm4OzLMsYY4xXbz9vvQVP4tO5B1y8l/+dewGc37Is4+bm5uQ1u/VXsvQTx+NxzPM8pmkau91u00AAnrd1XceyLONwOIz9/vRXms3BAYDf4U8DACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAAS3wFu9U8H7DZrnwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "filter = [np.array([\n",
    "    [1, 0, -1], [1, 0, -1], [1, 0, -1]\n",
    "])]\n",
    "\n",
    "#apply_filter_tf(images[0], filter, padding='VALID', strides=1)\n",
    "#apply_filter(images[0], filter, padding='VALID', strides=1)\n",
    "apply_filter_CNN2(images[0], filter, padding='VALID', strides=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 4, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAADaCAYAAACSN/T8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEK0lEQVR4nO3aMW7bSBiA0ZHgwg1zAEG6w5a5wfY5TwoX6XOD3Cn9djJ4gFUKAxuQW3mrQEiY7Cc4fq+eGfwFgQ/kcLeu6zoA4H+2v/UAALwOggNAQnAASAgOAAnBASAhOAAkBAeAxN3WjcuyjHmexzRNY7fb/cqZAHgh1nUdl8tlHA6Hsd9ff4fZHJx5nsfpdNq6HYDfyPl8Hsfj8eqazcGZpmmMMcbx4f3Y399vPQaAF2x5ehqPDx/+a8I1m4Pz/Bltf38vOACv3PdcrfhpAICE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4AibufPeDzu0/jzaRbt/Ln4Y9bj/Dq/fXx7a1HgBdBKQBICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABJ3Wzeu6zrGGOPvL8svG4Yf93X959YjvHrL09OtR4CbeX7+n5twzW79nlXf8Pj4OE6n05atAPxmzufzOB6PV9dsDs6yLGOe5zFN09jtdpsGBOBlW9d1XC6XcTgcxn5//ZZmc3AA4Ef4aQCAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQOJfZqpR7RWg1l4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 1. -1.  0.  0.]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "filters = [np.array([\n",
    "    [1, 0, -1], [0, 0, 0], [-1, 0, 1]\n",
    "])]\n",
    "\n",
    "#apply_filter_tf(images[0], filter, padding='VALID', strides=[1, 1, 1, 1])\n",
    "#apply_filter(images[0], filter, padding='VALID', strides=1)\n",
    "apply_filter_CNN2(images[0], filters, padding='VALID', strides=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 4, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAADaCAYAAACSN/T8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAESklEQVR4nO3cQYojZQCG4T+xxYVTvdJNSMCVO0/hAcRLeCgP4gkEdy68gaTJBQKD2j0pFzquJMyU7Vv05HnW9YdvEXipqpDNPM/zAID/2XbtAQDcBsEBICE4ACQEB4CE4ACQEBwAEoIDQOJu6cHL5TJOp9OYpmlsNpvn3ATACzHP8zifz2O3243t9vo9zOLgnE6ncTgclh4H4ANyPB7Hfr+/es3i4EzTNMYY49efvxj3rzyZW8u3X3619gTghj2Nx/Hj+OGfJlyzODhvH6Pdv9qO+0lw1nK3+XjtCcAt+/vP0d7l1YpSAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgcfdfP+DrX74ZH336yXNsYYHH7z5fe8LN++z7n9aeAC+COxwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4CE4ACQEBwAEoIDQEJwAEgIDgAJwQEgITgAJAQHgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0BCcABICA4ACcEBICE4ACQEB4DE3dKD8zyPMcZ48/r3ZxvD+3vzx29rT7h5T/Pj2hNgNU/jr+//2yZcs5nf5ap/8fDwMA6Hw5KjAHxgjsfj2O/3V69ZHJzL5TJOp9OYpmlsNptFAwF42eZ5Hufzeex2u7HdXn9Lszg4APA+/GgAgITgAJAQHAASggNAQnAASAgOAAnBASAhOAAkBAeAhOAAkBAcABKCA0DiT9+/Um8y3wv9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[3. 0. 0. 0.]\n",
      " [2. 1. 0. 0.]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "filters = [\n",
    "    np.array([[1, 1, 1], [0, 0, 0], [-1, -1, -1]])\n",
    "]\n",
    "\n",
    "#apply_filter_tf(images[0], filter, padding='VALID', strides=1)\n",
    "#apply_filter(images[0], filter, padding=0, strides=1)\n",
    "apply_filter_CNN2(images[0], filters, padding='VALID', strides=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 6, 1)\n",
      "tf.Tensor(\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]], shape=(4, 6), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 2. 0. 0. 0.]\n",
      " [0. 0. 4. 0. 0. 0.]\n",
      " [0. 0. 2. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0.]], shape=(4, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "filtros = [\n",
    "    np.array([[1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]]),   \n",
    "]\n",
    "\n",
    "apply_filter_CNN2(images[0], filtros, padding='SAME', strides=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV provides a multitude of techniques for digital image processing. Other examples of filters that we can apply to images are, for example, those that allow us to perform [morphological transformations] (https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ea9e838370>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kernel = np.ones((1, 1), np.uint8)\n",
    "dilation = cv2.dilate(img, kernel, iterations=1)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(5, 5))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(dilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is `scipy` does the mathematically 'correct' convolution, whereas `tensorflow` does the convolution oriented to a Convolutional Neural Network (CNN) application.\n",
    "\n",
    "Therefore, `scipy` inverts the kernel before applying the convolution (as explained [here](https://stackoverflow.com/questions/40247760/scipy-convolve2d-outputs-wrong-values)) whereas `tensorflow` does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_tf: tf.Tensor(\n",
      "[[-20.   0.  20.  20.   0.   0.]\n",
      " [-30.   0.  30.  30.   0.   0.]\n",
      " [-30.   0.  30.  30.   0.   0.]\n",
      " [-30.   0.  30.  30.   0.   0.]\n",
      " [-30.   0.  30.  30.   0.   0.]\n",
      " [-20.   0.  20.  20.   0.   0.]], shape=(6, 6), dtype=float32)\n",
      "sp_conv:\n",
      "[[ 20   0 -20 -20   0   0]\n",
      " [ 30   0 -30 -30   0   0]\n",
      " [ 30   0 -30 -30   0   0]\n",
      " [ 30   0 -30 -30   0   0]\n",
      " [ 30   0 -30 -30   0   0]\n",
      " [ 20   0 -20 -20   0   0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "img2 = np.array([\n",
    "    [10, 10, 10, 0, 0, 0],\n",
    "    [10, 10, 10, 0, 0, 0],\n",
    "    [10, 10, 10, 0, 0, 0],\n",
    "    [10, 10, 10, 0, 0, 0],\n",
    "    [10, 10, 10, 0, 0, 0],\n",
    "    [10, 10, 10, 0, 0, 0]\n",
    "]).astype(np.float32)\n",
    "k = np.array([\n",
    "    [1., 0., -1.],\n",
    "    [1., 0., -1.],\n",
    "    [1., 0., -1.]\n",
    "]).astype(np.float32)\n",
    "\n",
    "img_tf = tf.constant(tf.reshape(img2, (1, 6, 6, 1)), dtype=tf.float32)\n",
    "k_tf = tf.constant(tf.reshape(k, (3, 3, 1, 1)), dtype=tf.float32)\n",
    "conv_tf = tf.nn.conv2d(img_tf, k_tf, strides=[1, 1], padding=\"SAME\")[0, ..., 0]\n",
    "print(\"conv_tf: \" + str(conv_tf))\n",
    "\n",
    "np_conv = np.array(signal.convolve2d(img2 , k, \"same\"), np.int32)\n",
    "print(\"sp_conv:\\n\" + str(np_conv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(model,X_test,y_test):\n",
    "    results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Loss: {}'.format(results[0]))\n",
    "    print('Test Accuracy: {}'.format(results[1]))\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test_list = []\n",
    "    if isinstance(y_test, pd.DataFrame):\n",
    "        for i in range(0,len(y_test)):\n",
    "            y_test_list.append(y_test.iloc[i]['energia'])\n",
    "    elif isinstance(y_test, np.ndarray):\n",
    "        for i in range(0,len(y_test)):\n",
    "            y_test_list.append(y_test[i][0])\n",
    "    else:\n",
    "        for i in range(0,len(y_test)):\n",
    "            y_test_list.append(y_test.iloc[i])\n",
    "    try:\n",
    "        if y_pred.shape[1] == 1: y_pred = np.transpose(y_pred)[0]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    print(y_test_list,y_pred)\n",
    "    SSres = sum((y_test_list - y_pred)**2)\n",
    "    media = np.mean(y_test_list)\n",
    "    SStot = sum((y_test_list-media)**2)\n",
    "    R_cuadrado = 1- SSres/SStot\n",
    "    print('Coeficiente de correlacion: %s'%R_cuadrado)\n",
    "    plt.plot(y_pred)\n",
    "    plt.plot(y_test_list)\n",
    "    plt.show()\n",
    "    plt.scatter(y_test_list, y_pred)\n",
    "    plt.xlabel('True Values [Energy(kcal/mol)]')\n",
    "    plt.ylabel('Predictions [Energy(kcal/mol)]')\n",
    "    plt.axis('equal')\n",
    "    plt.axis('square')\n",
    "    plt.xlim([0,plt.xlim()[1]])\n",
    "    plt.ylim([0,plt.ylim()[1]])\n",
    "    _ = plt.plot([-100, 100], [-100, 100], 'r--')\n",
    "    plt.show()\n",
    "    error = y_pred - y_test_list\n",
    "    plt.hist(error, bins = 25)\n",
    "    plt.xlabel(\"Prediction Error [Energy(kcal/mol)]\")\n",
    "    _ = plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_loss_accuracy_evolution(history):\n",
    "    \n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Categorical Crossentropy')\n",
    "    ax1.plot(hist['epoch'], hist['loss'], label='Train loss')\n",
    "    ax1.plot(hist['epoch'], hist['val_loss'], label = 'Val loss')\n",
    "    ax1.grid()\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.plot(hist['epoch'], hist['mean_absolute_error'], label='Train Error')\n",
    "    ax2.plot(hist['epoch'], hist['val_mean_absolute_error'], label = 'Val Error')\n",
    "    ax2.grid()\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(real, predictions):\n",
    "    mse = mean_squared_error(real, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(real, predictions)\n",
    "    r2 = r2_score(real, predictions)\n",
    "\n",
    "    print('Mean Squared Error (MSE): ', mse)\n",
    "    print('Root Mean Squared Error (RMSE): ', rmse)\n",
    "    print('Mean Absolute Error (MAE): ', mae)\n",
    "    print('R-squared Score (R^2): ', r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='image_classification_cnn'></a>\n",
    "# Image Classification CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will study the problem of the additivity of the substituent effect with convolutional neural networks (CNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by constructing a neural network using 100% mono- and di-substituted compounds as training and validation. We will work with a set of **170 compounds** as a result of **5 different substituents** in **4 different positions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = images\n",
    "data_dir = \"Imagenes_1_2_sust\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a TensorFlow dataset with the data that we have previously loaded to disk with the [`image_dataset_from_directory ()`](https://keras.io/api/preprocessing/image/#image_dataset_from_directory-function)\n",
    "method.\n",
    "\n",
    "The `colos_mode` parameter (by default 'rgb') allows you to choose the color scale to use. To automatically load and convert the images to grayscale it must be set as `color_mode = grayscale`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files belonging to 0 classes.\n",
      "Using 0 files for training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No images found in directory Imagenes_1_2_sust. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19300\\3636320052.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimage_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n\u001b[0m\u001b[0;32m      3\u001b[0m   \u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# 80%  train, 20% validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# 'training' o 'validation', only  with 'validation_split'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m         )\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    295\u001b[0m                 \u001b[1;34mf\"No images found in directory {directory}. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m                 \u001b[1;34mf\"Allowed formats: {ALLOWLIST_FORMATS}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No images found in directory Imagenes_1_2_sust. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')"
     ]
    }
   ],
   "source": [
    "image_size = (4,6)\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,  # 80%  train, 20% validation\n",
    "  subset='training',  # 'training' o 'validation', only  with 'validation_split'\n",
    "  color_mode='grayscale',\n",
    "  seed=1,\n",
    "  image_size=image_size,  # Dimension (img_height, img_width) for rescaling\n",
    "  batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files belonging to 0 classes.\n",
      "Using 0 files for validation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No images found in directory Imagenes_1_2_sust. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19300\\529266235.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n\u001b[0m\u001b[0;32m      2\u001b[0m   \u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'validation'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m         )\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    295\u001b[0m                 \u001b[1;34mf\"No images found in directory {directory}. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m                 \u001b[1;34mf\"Allowed formats: {ALLOWLIST_FORMATS}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No images found in directory Imagenes_1_2_sust. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset='validation',\n",
    "  seed=1,\n",
    "  image_size=image_size,\n",
    "  batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_list = train_ds.class_names\n",
    "class_names_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the dataset for performance\n",
    "\n",
    "Let's make sure to use buffered prefetching so you can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data.\n",
    "\n",
    "`Dataset.cache()` keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n",
    "\n",
    "`Dataset.prefetch()` overlaps data preprocessing and model execution while training. \n",
    "\n",
    "Interested readers can learn more about both methods, as well as how to cache data to disk in the [data performance guide](https://www.tensorflow.org/guide/data_performance#prefetching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "# val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing some training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names_list[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the model. The **input will have a dimension of (n, n, 3)**, corresponding to (image height, image width, number of color channels).\n",
    "\n",
    "At the input of the network we include a preprocessing that will allow the images to be rescaled by normalizing the pixel values to a range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model in Keras\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://i.ibb.co/D8CmT6K/cnn.jpg\" alt=\"cnn\" border=\"0\">\n",
    "\n",
    "\n",
    "\n",
    "A Convolutional Neural Network (CNN) architecture has four main parts:\n",
    "\n",
    "- A **convolutional layer** that extracts features from a source image. \n",
    "\n",
    "- A **pooling layer** that reduces the image dimensionality without losing important features or patterns.\n",
    "\n",
    "- A **flattening layer** that transforms a n-dimensional tensor into a vector that can be fed into a fully connected neural network.\n",
    "\n",
    "- A **fully connected layer** also known as the dense layer.\n",
    "\n",
    "### Rescaling\n",
    "\n",
    "For converting the images to   \\[0,1\\] range.\n",
    "```python\n",
    "normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)\n",
    "next_layer = normalization_layer(prev_layer)\n",
    "```\n",
    "or simply\n",
    "```python\n",
    "reescaling = layers.experimental.preprocessing.Rescaling(1. / 255)(inputs)\n",
    "```\n",
    "\n",
    "### Convolutional layer\n",
    "\n",
    "In the convolutional layers (`Conv2D`) we will configure the following parameters:\n",
    "\n",
    "- **filters**: number of feature maps.\n",
    "- **kernel_size**: can be either an integer or a tuple of two integers. Specifies the height and width of the kernel.\n",
    "- **padding**: allows you to include padding in the input data. With 'valid' it is not applied, with 'same' it is configured so that the dimension at the output of the convolution is the same as at the input.\n",
    "- **activation**: activation function implemented. Recommended ReLU.\n",
    "\n",
    "[Link to documentation](https://keras.io/api/layers/convolution_layers/convolution2d/)\n",
    "\n",
    "```python\n",
    "tf.keras.layers.Conv2D(\n",
    "    filters, kernel_size, strides=(1, 1), padding='valid',\n",
    "    activation=None, kernel_regularizer=None)\n",
    "\n",
    "```\n",
    "\n",
    "With Functional API:\n",
    "```python\n",
    "next_layer = layers.Conv2D(filters=8, kernel_size=3, activation='relu', name='conv_1')(prev_layer)\n",
    "```\n",
    "\n",
    "With Sequential:\n",
    "```python\n",
    "model.add(layers.Conv2D(filters=8,kernel_size=3, activation='relu', name='conv_1'))\n",
    "```\n",
    "\n",
    "### Pooling layer\n",
    "\n",
    "A pooling layer is a new layer added after the convolutional layer. Specifically, after a nonlinearity ( ReLU) you can choose between [average pooling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D) or [max pooling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D). Usually max pooling is the best choice.\n",
    "\n",
    "\n",
    "With Functional API:\n",
    "```python\n",
    "conv_1 = layers.Conv2D(filters=8, kernel_size=3, activation='relu', name='conv_1')(prev_layer)\n",
    "\n",
    "pool_1 = layers.MaxPool2D(pool_size=(2, 2), name='pool_1')(conv_1)\n",
    "```\n",
    "\n",
    "With Sequential:\n",
    "```python\n",
    "model.add(layers.AveragePooling2D(pool_size=(2, 2), name='pool_1'))\n",
    "```\n",
    "\n",
    "### Flattening\n",
    "\n",
    "Prepares a vector for the fully connected layers.\n",
    "\n",
    "With Functional API:\n",
    "\n",
    "```python\n",
    "next_layer = layers.Flatten(name='flatten')(prev_layer)\n",
    "```\n",
    "\n",
    "With Sequential:\n",
    "```python\n",
    "model.add(layers.Flatten(name='flatten'))\n",
    "```\n",
    "\n",
    "There is another alternative for flattening that is a type of pooling that is called global pooling. Global pooling down-samples the entire feature map to a single value. \n",
    "\n",
    "You can also choose between [GlobalAveragePooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D) and [GlobalMaxPooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool2D).\n",
    "\n",
    "```python\n",
    "model.add(layers.GlobalMaxPool2D(name='GlobalMaxPooling2D'))\n",
    "```\n",
    "\n",
    "### Fully-connected layer\n",
    "\n",
    "Dense layer like a simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ConvNN(filters=51, kernel_size=(7,7), unit1=168, unit2=157, unit3=205, unit4=12, dropout1=0.08161034482758621, dropout2=0.18786206896551724, dropout3=0.42844827586206896): \n",
    "    \n",
    "    model = Sequential()\n",
    "    #model.add(Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None))\n",
    "    #model.add(Conv2D(filters=64, kernel_size=7, activation='relu', input_shape=(7, 42, 1) ))\n",
    "    model.add(Conv2D(filters=filters, kernel_size=kernel_size, activation='relu', input_shape=(4, 6, 1) ))\n",
    "\n",
    "    model.add(Conv2D(64, 3, activation='relu'))\n",
    "    #model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, 1, activation='relu'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #model.add(Dense(units=294, activation='relu', input_dim=294))\n",
    "    #model.add(Dense(units=256, activation='relu'))\n",
    "    model.add(Dense(units=unit1, activation='relu'))\n",
    "    model.add(Dropout(dropout1)) ##\n",
    "    model.add(Dense(units=unit2, activation='relu'))\n",
    "    model.add(Dropout(dropout2)) \n",
    "    model.add(Dense(units=unit3, activation='relu'))\n",
    "    model.add(Dropout(dropout3)) ##\n",
    "    model.add(Dense(units=unit4, activation='relu'))\n",
    "    model.add(Dense(units=1, activation='linear'))                         #ingen aktivering til sidste lag, fordi vi √É¬∏nsker den fulde v√É¬¶rdi i outputtet\n",
    "    #model.add(Dense(units=1))\n",
    "\n",
    "    #model.compile(loss='logcosh', optimizer='adam', metrics=['accuracy']) #accuracy er en metric til classification\n",
    "    model.compile(loss='logcosh', optimizer='Nadam', metrics=['mean_absolute_error'])\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 3, 5, 51)          255       \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 1, 3, 64)          29440     \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 1, 3, 128)         8320      \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 128)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 168)               21672     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 168)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 157)               26533     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 157)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 205)               32390     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 205)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 12)                2472      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 121,095\n",
      "Trainable params: 121,095\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNN(filters=51, kernel_size=(2,2), unit1=168, unit2=157, unit3=205, unit4=12, dropout1=0.08161034482758621, dropout2=0.18786206896551724, dropout3=0.42844827586206896)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot bad predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = {layer.name: layer.output for layer in model_VGG16.layers}\n",
    "# Set up a model that returns the activation values for every layer in\n",
    "# VGG19 (as a dict).\n",
    "feature_extractor = keras.Model(inputs=model_VGG16.inputs,\n",
    "                                outputs=outputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the features of the image\n",
    "features = feature_extractor(img)\n",
    "print(features.keys())\n",
    "#print(list(features.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature map for first hidden layer\n",
    "feature_maps = features['block1_conv1']\n",
    "print('feature_maps first hidden layer shape: ', feature_maps.shape)\n",
    "# plot all 64 maps in an 8x8 squares\n",
    "square = 8\n",
    "ix = 1\n",
    "plt.figure(figsize=(30, 30))\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in viridis or gray\n",
    "        plt.imshow(feature_maps[0, :, :, ix - 1], cmap='viridis')\n",
    "        ix += 1\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block3_conv3\n",
    "feature_maps = features['block3_conv3']\n",
    "print('feature_maps shape: ', feature_maps.shape)\n",
    "\n",
    "# plot all 256 maps in an 16x16 squares\n",
    "square = 16\n",
    "ix = 1\n",
    "plt.figure(figsize=(40, 40))\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(feature_maps[0, :, :, ix - 1], cmap='viridis')\n",
    "        ix += 1\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = features['block1_conv2'] \n",
    "print('feature_maps shape: ', feature_maps.shape)\n",
    "for i in range(feature_maps.shape[-1]):\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = features['block1_pool'] \n",
    "print('feature_maps shape: ', feature_maps.shape)\n",
    "for i in range(feature_maps.shape[-1]):\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: What happen with the validation loss and with the number of parameters if you increment the number of filters and the kernel_size ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=image_size + (3, ), name='input')\n",
    "reescaling = layers.experimental.preprocessing.Rescaling(1. / 255)(inputs)\n",
    "\n",
    "# Conv Layer 1\n",
    "conv_1 = layers.Conv2D(filters=..., kernel_size=..., padding='valid', activation='relu',\n",
    "                       name='conv_1')(reescaling)\n",
    "pool_1 = layers.MaxPooling2D(pool_size=(2, 2), name='pool_1')(conv_1)\n",
    "\n",
    "# Conv Layer 2\n",
    "conv_2 = layers.Conv2D(..., ..., padding='valid', activation='relu',\n",
    "                       name='conv_2')(pool_1)\n",
    "pool_2 = layers.MaxPooling2D(pool_size=(2, 2), name='pool_2')(conv_2)\n",
    "\n",
    "\n",
    "# Fully-connected\n",
    "# Flattening\n",
    "flat = layers.Flatten(name='flatten')(pool_2)\n",
    "dense = layers.Dense(64, activation='relu', name='dense')(flat)\n",
    "outputs = layers.Dense(5, activation='softmax', name='output')(dense)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='cnn_example')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    ")\n",
    "show_loss_accuracy_evolution(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: What happen with the validation loss and with the number of parameters if you vary  the `pool_size` and the padding of the filters to `same` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=image_size + (3, ), name='input')\n",
    "reescaling = layers.experimental.preprocessing.Rescaling(1. / 255)(inputs)\n",
    "\n",
    "# Conv Layer 1\n",
    "conv_1 = layers.Conv2D(filters=..., kernel_size=..., padding=..., activation='relu',\n",
    "                       name='conv_1')(reescaling)\n",
    "pool_1 = layers.MaxPooling2D(pool_size=(..., ...), name='pool_1')(conv_1)\n",
    "\n",
    "# Conv Layer 2\n",
    "conv_2 = layers.Conv2D(4, 3, padding='valid', activation='relu',\n",
    "                       name='conv_2')(pool_1)\n",
    "pool_2 = layers.MaxPooling2D(pool_size=(2, 2), name='pool_2')(conv_2)\n",
    "\n",
    "# Fully-connected\n",
    "# Flattening\n",
    "flat = layers.Flatten(name='flatten')(pool_2)\n",
    "dense = layers.Dense(64, activation='relu', name='dense')(flat)\n",
    "outputs = layers.Dense(5, activation='softmax', name='output')(dense)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='cnn_example')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    ")\n",
    "show_loss_accuracy_evolution(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Create a model with `val_accuracy > 0.72`\n",
    "\n",
    "You can use early-stopping callback and dropouts techniques.\n",
    "\n",
    "```python\n",
    "next_layer = layers.Dropout(0.4)(prev_layer)\n",
    "```\n",
    "\n",
    "```python\n",
    "next_layer = layers.BatchNormalization()(prev_layer)\n",
    "```\n",
    "\n",
    "```python\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # can be 'val_accuracy'\n",
    "    patience=5,  # if during 5 epochs there is no improvement in `val_loss`, the execution will stop\n",
    "    verbose=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=image_size + (3, ), name='input')\n",
    "reescaling = layers.experimental.preprocessing.Rescaling(1. / 255)(inputs)\n",
    "...\n",
    "outputs = layers.Dense(5, activation='softmax', name='output')(dense)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='cnn_example')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # can be 'val_accuracy'\n",
    "    patience=4,  # if during 5 epochs there is no improvement in `val_loss`, the execution will stop\n",
    "    verbose=1)\n",
    "\n",
    "\n",
    "epochs = 25\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=[es_callback]\n",
    ")\n",
    "\n",
    "show_loss_accuracy_evolution(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_errors(val_ds, model, class_names_list, n_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "[**data augmentation**](https://en.wikipedia.org/wiki/Data_augmentation) We transform randomly the training images.\n",
    "\n",
    "Data augmentation can be done before starting any training directly on the available image set, or working with the [Keras layers for that purpose](https://www.tensorflow.org/tutorials/images/data_augmentation). In that example we will do it the second way by randomly rotating, flipping and scaling the images. You can choose lot of techniques:\n",
    "\n",
    "- [layers.experimental.preprocessing.RandomFlip](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomFlip): Randomly flip each image horizontally and vertically.\n",
    "\n",
    "- [layers.experimental.preprocessing.RandomContrast](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomContrast): Adjust the contrast of an image or images by a random factor.\n",
    "\n",
    "- [layers.experimental.preprocessing.RandomRotation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomRotation): Randomly rotate each image.\n",
    "\n",
    "- [layers.experimental.preprocessing.RandomZoom](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomZoom): Randomly zoom each image during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, _ in train_ds.take(3):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    img = images[0:1]\n",
    "    ax = plt.subplot(1, 5, 1)\n",
    "    plt.imshow(img[0].numpy().astype(\"uint8\"))\n",
    "    plt.title('Original Image')\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    ax = plt.subplot(1, 5, 2)\n",
    "    img_aug = layers.experimental.preprocessing.RandomFlip(\n",
    "        \"horizontal_and_vertical\")(img)\n",
    "    plt.imshow(img_aug[0].numpy().astype(\"uint8\"))\n",
    "    plt.title('Random Flip')\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    ax = plt.subplot(1, 5, 3)\n",
    "    img_aug = layers.experimental.preprocessing.RandomRotation(0.25)(img)\n",
    "    plt.imshow(img_aug[0].numpy().astype(\"uint8\"))\n",
    "    plt.title('Random Rotation')\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    ax = plt.subplot(1, 5, 4)\n",
    "    img_aug = layers.experimental.preprocessing.RandomZoom(0.65)(img)\n",
    "    plt.imshow(img_aug[0].numpy().astype(\"uint8\"))\n",
    "    plt.title('Random Zoom')\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    ax = plt.subplot(1, 5, 5)\n",
    "    img_aug = tf.keras.layers.experimental.preprocessing.RandomContrast(\n",
    "        0.25)(img)\n",
    "    plt.imshow(img_aug[0].numpy().astype(\"uint8\"))\n",
    "    plt.title('Random Contrast')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can put all together\n",
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.experimental.preprocessing.RandomFlip(),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.25),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.25),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include the stage that performs the transformations in the images at the beginning of the model that we built previously, we repeat the training and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=image_size + (3, ), name='input')\n",
    "data_aug = data_augmentation(inputs)\n",
    "reescaling = layers.experimental.preprocessing.Rescaling(1. / 255)(data_aug)\n",
    "\n",
    "# Conv Layer 1\n",
    "conv_1 = layers.Conv2D(8, 3, padding='valid', activation='relu',\n",
    "                       name='conv_1')(reescaling)\n",
    "pool_1 = layers.MaxPooling2D(pool_size=(2, 2), name='pool_1')(conv_1)\n",
    "\n",
    "# Conv Layer 2\n",
    "conv_2 = layers.Conv2D(8, 3, padding='valid', activation='relu',\n",
    "                       name='conv_2')(pool_1)\n",
    "pool_2 = layers.MaxPooling2D(name='pool_2')(conv_2)\n",
    "\n",
    "# Conv Layer 3\n",
    "conv_3 = layers.Conv2D(8,\n",
    "                       3,\n",
    "                       padding='valid',\n",
    "                       activation='relu',\n",
    "                       name='conv_3')(pool_2)\n",
    "pool_3 = layers.MaxPooling2D(name='pool_3')(conv_3)\n",
    "\n",
    "# Fully-connected\n",
    "flat = layers.Flatten(name='flatten')(pool_3)\n",
    "dense = layers.Dense(64, activation='relu', name='dense')(flat)\n",
    "outputs = layers.Dense(5, activation='softmax', name='output')(dense)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='cnn_example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # can be 'val_accuracy'\n",
    "    patience=10,  # if during 5 epochs there is no improvement in `val_loss`, the execution will stop\n",
    "    verbose=1)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=[es_callback]\n",
    ")\n",
    "show_loss_accuracy_evolution(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Use the best model you have found and include the `data_aug` layer, compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=image_size + (3, ), name='input')\n",
    "data_aug = data_augmentation(inputs)\n",
    "reescaling = layers.experimental.preprocessing.Rescaling(1. / 255)(data_aug)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # can be 'val_accuracy'\n",
    "    patience=10,  # if during 5 epochs there is no improvement in `val_loss`, the execution will stop\n",
    "    verbose=1)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "epochs = 50\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=[es_callback]\n",
    ")\n",
    "show_loss_accuracy_evolution(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_errors(val_ds, model, class_names_list, n_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "\n",
    "With transfer learning, you benefit from both advanced convolutional neural network architectures developed by top researchers and from pre-training on a huge dataset of images. In our case we will be transfer learning from a network trained on ImageNet, a database of images containing many plants and outdoors scenes, which is close enough to flowers.\n",
    "\n",
    "<img src=\"https://i.ibb.co/KsLSGyt/transfer-learning.png\" alt=\"transfer-learning\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MobileNetV2](https://arxiv.org/abs/1801.04381) is a significant improvement over MobileNetV1 and pushes the state of the art for mobile visual recognition including classification, object detection and semantic segmentation.\n",
    "\n",
    "In [`tf.keras.applications`](https://www.tensorflow.org/api_docs/python/tf/keras/applications) you have many pre-trained models. You can compare them [here](https://keras.io/api/applications/#available-models).\n",
    "\n",
    "With the parameter `include_top=False`, you can delete the last `softmax` layer.\n",
    "\n",
    "With `pretrained_model.trainable = False`, you freeze the pre-trained model weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = tf.keras.applications.MobileNetV2(input_shape=image_size+(3,), include_top=False)\n",
    "pretrained_model.trainable = False\n",
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use the same **preprocessing** as in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You define your new model adding more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=image_size + (3, ), name='input')\n",
    "\n",
    "# pre-trained model\n",
    "x = preprocess_input(inputs)\n",
    "x = pretrained_model(x)\n",
    "\n",
    "# classifier\n",
    "flat = tf.keras.layers.Flatten()(x)\n",
    "flat = tf.keras.layers.Dropout(0.5)(flat)\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax')(flat)\n",
    "\n",
    "model_tl = tf.keras.Model(inputs, outputs)\n",
    "model_tl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tl.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "history = model_tl.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss_accuracy_evolution(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_errors(val_ds, model_tl, class_names_list, n_images=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model_tl, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=image_size+(3, ), name='input')\n",
    "\n",
    "#data_aug = data_augmentation(inputs)\n",
    "reescaling = preprocess_input(inputs)\n",
    "\n",
    "# Conv Layer 1\n",
    "conv_1 = layers.Conv2D(16, 3, padding='valid',\n",
    "                       activation='relu', name='conv_1')(reescaling)\n",
    "pool_1 = layers.MaxPooling2D(pool_size=(\n",
    "    2, 2),  name='pool_1')(conv_1)\n",
    "pool_1 = layers.Dropout(0.4)(pool_1)\n",
    "\n",
    "# Conv Layer 2\n",
    "conv_2 = layers.Conv2D(16, 3, padding='valid',\n",
    "                       activation='relu', name='conv_2')(pool_1)\n",
    "pool_2 = layers.MaxPooling2D(name='pool_2')(conv_2)\n",
    "pool_2 = layers.Dropout(0.4)(pool_2)\n",
    "\n",
    "# Conv Layer 3\n",
    "conv_3 = layers.Conv2D(16, 3, padding='valid',\n",
    "                       activation='relu', name='conv_3')(pool_2)\n",
    "pool_3 = layers.MaxPooling2D(name='pool_3')(conv_3)\n",
    "pool_3 = layers.Dropout(0.4)(pool_3)\n",
    "\n",
    "# Fully-connected\n",
    "x1 = layers.Flatten(name='flatten')(pool_3)\n",
    "\n",
    "x2 = pretrained_model(reescaling, training=False)\n",
    "x2 = tf.keras.layers.Flatten()(x2)\n",
    "\n",
    "x = tf.keras.layers.Concatenate()([x1, x2])\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(64, activation='relu', name='dense')(x)\n",
    "# A Dense classifier with a single unit (binary classification\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax')(x)\n",
    "model_tl = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model_tl, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tl.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "epochs = 15\n",
    "history = model_tl.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_errors(val_ds, model_tl, class_names_list, n_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "model_tl = tf.keras.Sequential([\n",
    "    data_augmentation,\n",
    "    layers.experimental.preprocessing.Rescaling(1./255),\n",
    "    tf.keras.applications.MobileNetV2(input_shape=image_size+(3, ), include_top=False),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "model_tl.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history = model_tl.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice: Fashion MNIST Try to obtain  `Test Accuracy>0.89`!!\n",
    "\n",
    "Fashion MNIST dataset contains 70,000 grayscale images with 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels).\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"300\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "**Categories**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Label</th>\n",
    "    <th>Class</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>T-shirt/top</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Trouser</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>Pullover</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3</td>\n",
    "    <td>Dress</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>4</td>\n",
    "    <td>Coat</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>Sandal</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>6</td>\n",
    "    <td>Shirt</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>7</td>\n",
    "    <td>Sneaker</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>8</td>\n",
    "    <td>Bag</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>9</td>\n",
    "    <td>Ankle boot</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images,\n",
    "                               test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "print('train_images shape: {0}, test_images shape: {1}'.format(\n",
    "    train_images.shape, test_images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt',\n",
    "    'Sneaker', 'Bag', 'Ankle boot'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap='gray')\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.experimental.preprocessing.RandomFlip(),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.25),\n",
    "  ]\n",
    ")\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.experimental.preprocessing.RandomFlip(),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(28, 28, 1), name='input')\n",
    "#aug = data_augmentation(inputs)\n",
    "reescaling = layers.experimental.preprocessing.Rescaling(1. / 255)(inputs)\n",
    "...\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='cnn_mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=7, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_images, test_labels, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)\n",
    "predicted_classes = np.argmax(predictions, -1)\n",
    "predictions.shape, predicted_classes.shape\n",
    "predictions[i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 5\n",
    "W = 5\n",
    "fig, axes = plt.subplots(L, W, figsize=(18, 18))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in np.arange(0, L * W):\n",
    "    axes[i].imshow(test_images[i].reshape(28, 28))\n",
    "    prob_pred = np.max(predictions[i, :])\n",
    "    class_pred = class_names[int(predicted_classes[i])]\n",
    "    original_class = class_names[int(test_labels[i])]\n",
    "    if class_pred == original_class:\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    title = \"Pred: {0} \\n Target: {1} \\n Prob: {2:.3f}\".format(\n",
    "    class_pred, original_class, prob_pred)\n",
    "    axes[i].set_title(title, color=color)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
